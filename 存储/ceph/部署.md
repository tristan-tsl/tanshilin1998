提前手动重置掉不需要的硬盘
wipe --all /dev/sda


部署

清单文件目录: `ceph\rook-1.7.3\cluster\examples\kubernetes\ceph`

```
kubectl apply -f crds.yaml
kubectl apply -f common.yaml
kubectl apply -f operator.yaml
kubectl apply -f cluster.yaml
```



查看情况

operator

```
kubectl -n rook-ceph get deployment
kubectl -n rook-ceph get pod -o wide|grep rook-ceph-operator
kubectl -n rook-ceph describe pod rook-ceph-operator-0

kubectl -n rook-ceph logs -f --tail 100 rook-ceph-operator-0
```

cluster

```
# mon
kubectl -n rook-ceph get pod -o wide|grep mon
kubectl -n rook-ceph describe pod rook-ceph-mon-a-64f84f7759-n9nws
kubectl -n rook-ceph logs -f --tail 100 rook-ceph-mon-a-64f84f7759-n9nws
kubectl -n rook-ceph logs -f --tail 10000 rook-ceph-mon-b-5fcc58888d-kwzbl
kubectl -n rook-ceph logs -f --tail 100 rook-ceph-mon-c-5bcb88978b-5wvfd

kubectl -n rook-ceph delete pod rook-ceph-mon-d-6767dd54b5-zvnd6
kubectl -n rook-ceph exec -it rook-ceph-mon-d-6c97b4bc9c-59rp2 -c mon -- bash


# mgr
kubectl -n rook-ceph get pod -o wide|grep mgr
kubectl -n rook-ceph logs -f --tail 100 rook-ceph-mgr-b-7998fdf778-nrzz6 mgr
kubectl -n rook-ceph logs -f --tail 100 rook-ceph-mgr-b-7998fdf778-r4tp4 mgr

kubectl -n rook-ceph delete pod rook-ceph-mgr-b-7998fdf778-r4tp4


# osd 主机数量x硬盘数量
kubectl -n rook-ceph get pod -o wide|grep osd

kubectl -n rook-ceph describe pod rook-ceph-osd-prepare-192.168.90.22-hb7x2
kubectl -n rook-ceph logs -f --tail 100 rook-ceph-osd-prepare-192.168.90.16-qcs88
kubectl -n rook-ceph logs -f --tail 100 rook-ceph-osd-1-f8bf76cf6-kv8x9
kubectl -n rook-ceph logs -f --tail 100 rook-ceph-osd-2-f5b669949-hrsw4
kubectl -n rook-ceph logs -f --tail 100 rook-ceph-osd-prepare-192.168.90.14-7ftrb
kubectl -n rook-ceph delete job rook-ceph-osd-prepare-192.168.90.16
kubectl -n rook-ceph delete pod rook-ceph-osd-prepare-192.168.90.14-lv46m

kubectl -n rook-ceph exec -it rook-ceph-osd-prepare-192.168.90.14-z4b7b -- bash


kubectl -n rook-ceph get job|grep osd
kubectl -n rook-ceph edit job rook-ceph-osd-prepare-192.168.90.14
kubectl -n rook-ceph edit job rook-ceph-osd-prepare-192.168.90.17




# rook-ceph-crashcollector
kubectl -n rook-ceph get pod -o wide|grep rook-ceph-crashcollector
kubectl -n rook-ceph logs -f --tail 100 rook-ceph-crashcollector-192.168.90.11-77f77447fb-k4rp9
kubectl -n rook-ceph logs -f --tail 100 rook-ceph-crashcollector-192.168.90.14-5fb4cc9957-h9kkj
kubectl -n rook-ceph logs -f --tail 100 rook-ceph-crashcollector-192.168.90.16-5585976df9-s8vsc


# csi-cephfsplugin-provisioner
kubectl -n rook-ceph get pod -o wide|grep csi-cephfsplugin-provisioner
kubectl -n rook-ceph delete pod csi-cephfsplugin-provisioner-ddbd67956-k4c8w
kubectl -n rook-ceph describe pod csi-cephfsplugin-provisioner-ddbd67956-k4c8w
kubectl -n rook-ceph logs -f --tail 100 csi-cephfsplugin-provisioner-67cdc46bc5-l7xmc csi-attacher
kubectl -n rook-ceph logs -f --tail 100 csi-cephfsplugin-provisioner-67cdc46bc5-l7xmc csi-snapshotter
kubectl -n rook-ceph logs -f --tail 100 csi-cephfsplugin-provisioner-67cdc46bc5-l7xmc csi-resizer
kubectl -n rook-ceph logs -f --tail 100 csi-cephfsplugin-provisioner-67cdc46bc5-l7xmc csi-provisioner
kubectl -n rook-ceph logs -f --tail 100 csi-cephfsplugin-provisioner-67cdc46bc5-l7xmc csi-cephfsplugin
kubectl -n rook-ceph logs -f --tail 100 csi-cephfsplugin-provisioner-67cdc46bc5-l7xmc liveness-prometheus


```



grafana

```
# 安装插件
kubectl -n monitoring get pod|grep grafana
kubectl -n monitoring describe pod grafana-0
kubectl -n monitoring logs -f --tail 100 grafana-0
kubectl -n monitoring exec -it grafana-0 -- bash

grafana-cli plugins install vonage-status-panel
grafana-cli plugins install grafana-piechart-panel

kubectl -n monitoring delete pod grafana-0
```

如何新加磁盘
```shell
kubectl -n rook-ceph get pod -o wide |grep rook-ceph-operator
kubectl -n rook-ceph describe pod rook-ceph-operator-0
kubectl -n rook-ceph edit pod rook-ceph-operator-0
kubectl -n rook-ceph delete pod rook-ceph-operator-cdf9dfd9c-srgzq
kubectl -n rook-ceph logs -f --tail 100 deployment/rook-ceph-operator

kubectl -n rook-ceph exec -it rook-ceph-operator-0 -- bash
```


如何删除磁盘
```shell
kubectl -n rook-ceph get pod -o wide|grep tool
kubectl -n rook-ceph exec -it rook-ceph-tools-5b54fb98c-2kzqm -- bash
ceph osd status

# 删除磁盘
ceph osd out osd.0
ceph osd purge 3 --yes-i-really-mean-it
ceph osd crush remove 0
# ceph auth rm osd.4
# ceph osd rm 4
# 观察数据迁移过程
ceph -w
```

在实际应用场景中, 需要划分两种storageClass, 应用层如果有高可用方案则使用副本数为1的sc, 反之使用副本数为2的sc